
### Sources
https://plato.stanford.edu/entries/information/#DeteTher
https://plato.stanford.edu/entries/church-turing/
https://plato.stanford.edu/entries/information-entropy/
https://plato.stanford.edu/entries/computer-science/
https://plato.stanford.edu/entries/computational-complexity/




# NOTES


## Information

### - **Qualitative Theories of Information**
 1. **Semantic Information:** Bar-Hillel and Carnap developed a theory of semantic Information (1953). Floridi (2002, 2003, 2011) defines semantic information as well-formed, meaningful and truthful data. Formal entropy based definitions of information (Fisher, Shannon, Quantum, Kolmogorov) work on a more general level and do not necessarily measure information in meaningful truthful datasets, although one might defend the view that in order to be measurable the data must be well-formed (for a discussion see [section 6.6 on Logic and Semantic Information](https://plato.stanford.edu/entries/information/#LogiSemaInfo)). Semantic information is close to our everyday naive notion of information as something that is conveyed by true statements about the world.
2. **Information as a state of an agent:** the formal logical treatment of notions like knowledge and belief was initiated by Hintikka (1962, 1973). Dretske (1981) and van Benthem & van Rooij (2003) studied these notions in the context of information theory, cf. van Rooij (2003) on questions and answers, or Parikh & Ramanujam (2003) on general messaging. Also Dunn seems to have this notion in mind when he defines information as “what is left of knowledge when one takes away belief, justification and truth” (Dunn 2001: 423; 2008). Vigo proposed a Structure-Sensitive Theory of Information based on the complexity of concept acquisition by agents (Vigo 2011, 2012).

### **Quantitative Theories of Information**
1. **Nyquist’s function:** Nyquist (1924) was probably the first to express the amount of “intelligence” that could be transmitted given a certain line speed of a telegraph systems in terms of a log function: W=klogm�=�log⁡�, where _W_ is the speed of transmission, _K_ is a constant, and _m_ are the different voltage levels one can choose from.
2. **Fisher information:** the amount of information that an observable random variable _X_ carries about an unknown parameter θ� upon which the probability of _X_ depends (Fisher 1925).
3. **The Hartley function:** (Hartley 1928, Rényi 1961, Vigo 2012). The amount of information we get when we select an element from a finite set _S_ under uniform distribution is the logarithm of the cardinality of that set.
4. **Shannon information:** the entropy, _H_, of a discrete random variable _X_ is a measure of the amount of uncertainty associated with the value of _X_ (Shannon 1948; Shannon & Weaver 1949).
5. **Kolmogorov complexity:** the information in a binary string _x_ is the length of the shortest program _p_ that produces _x_ on a reference universal Turing machine _U_ (Turing 1937; Solomonoff 1960, 1964a,b, 1997; Kolmogorov 1965; Chaitin 1969, 1987).
6. **Entropy measures in Physics:** Although they are not in all cases strictly measures of information, the different notions of entropy defined in physics are closely related to corresponding concepts of information. We mention _Boltzmann Entropy_ (Boltzmann, 1866) closely related to the Hartley Function (Hartley 1928), _Gibbs Entropy_ (Gibbs 1906) formally equivalent to Shannon entropy and various generalizations like _Tsallis Entropy_ (Tsallis 1988) and _Rényi Entropy_ (Rényi 1961).
7. **Quantum Information:** The qubit is a generalization of the classical bit and is described by a quantum state in a two-state quantum-mechanical system, which is formally equivalent to a two-dimensional vector space over the complex numbers (Von Neumann 1932; Redei & Stöltzner 2001)




## Complexity



